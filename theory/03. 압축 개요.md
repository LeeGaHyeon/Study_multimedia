## 03. 압축 개요
- 학습목표
  - 압축 개요
  - 앤트로피 개념
  
- 데이터 압축이란?
  - 대용량 데이터의 문제점
    - 저장 공간의 관점
      - 더욱 많은 메모리 공간을 필요로 함
      - 데이터 접근이나 탐색을 위한 시간 소요가 늘어남
      - 저장공간의 제약이 있는 모바일 기기에서 사용이 불편함
    - 데이터 통신의 관점
      - 데이터 용량에 비례한 전송시간이 필요함
      - 실시간 통신 어려움 (예: 스트리밍)
      
 #### 압축 : 데이터를 더 적은 저장 공간에 효율적으로 기록하기 위한 기술 
 
 - 압축 개요
  - 용어 (terminology)
    - 기호/심볼(Symbol) : 원본의 정보를 나타내는 기본 요소 
    - 코드워드(codeword) : 심볼로부터 매핑된 코드의 출력을 나타냄
   - 압축의 정의
    - 인코드(encoder)를 통해 원본 데이터 심볼(symbol)에서부터 인코딩 된 코드워드의 평균 길이를 최소화 하는 과정 
    - 압축률 = 원본 데이터 사이즈/압축 데이터 사이즈 
 
 - 엔트로피 부호화(entropy coding)
  - 데이터를 손실 없이 압축(lossless data compression)하는 방법
  - 텍스트뿐만 아니라 영상이나 비디오 등의 다양한 데이터에 적용 가능 
  - 주요 기법 : 허프만 부호화(Huffman coding)와 산술 부호화(archimetic coding)

- 엔트로피 개요
  - 엔트로피(entropy)
    - 확률 모델에서 발생하는 각 데이터의 정보량(amount of information)을 표현하는 수치
  - 예시 : 일기예보
    - 내일 밤에는 해가 지고 어둡습니다. => 무용지물인 정보(발생확률 100%)
    - 내일 밤에는 눈이 10cm 내리겠습니다. => 의미 있는 정보(20%)
    - 내일 밤에는 월식이 발생합니다. => 매우 귀중한 정보(0.001%)
  - 자주 발생하는 사건은 낮은 정보량을 가진다. 발생이 보장된 사건은 그 내용에 상관없이 전혀 정보가 없다는 걸 뜻한다.
  - 덜 자주 발생하는 사건은 더 높은 정보량을 가진다. 
  - 독립사건(independent event)은 추가적인 정보량(additive information)을 가진다. 예컨데 동전을 던져 앞면이 두번 나오는 사건에 대한 정보량은 동전을 던져 앞면이 한번 나오는 정보량의 두 배이다.
  - 스무고개 예시 
    - 문제. 고양이와 아르마딜로 
 
 - 엔트로피(1)
  - 불확실성 (uncertainty 또는 surprisal)이 커질수록 엔트로피는 커지고 포함하는 정보의 양은 증가
    - 낮은 확률을 갖는 불확실한 사건일수록 엔트로피가 높아짐
    - 발생 확률과 정보의 양은 반비례의 관계

  ### 정보 엔트로피는 데이터를 압축하는 데 필요한 최대 데이터양을 계산하는 근거를 제시한다.
    - 무손실 압축의 경우 각 기호는 적어도 엔트로피 만큼의 비트수가 필요함
    
  - 엔트로피(2)
    - 기호 k의 불확실성 Uk는 Uk = log2 1/Pk = -log2 Pk
    - 확률 모델에서의 최소 평균 정보량인 엔트로피 H = -시그마(n~k)Pk*log2 Pk
    - 예 : 공정한 동전 던지기의 경우 앞면의 불확실성 = -log2 Pk = -log2 1/2 = 1비트 
    - 공정한 동전 던지기를 위한 엔트로피 = 1x1/2+1x1/2=1
   
  - 엔트로피(3)

  - 엔트로피(4)
    - 각 기호는 8비트로 표현할 수 있으며 80번의 문자가 등장하므로 모든 데이터를 저장하기 위해서는 640비트(8비트x80)가 필요 
    - 엔트로피를 계산하면 2.260178(비트)
      - 각 기호 당 최대 약 2.3비트를 사용하면 표현이 가능한 것을 의미
      - 데이터를 저장하기 위해 필요한 용량은 180.81(2.260178비트x80)비트
    - 엔트로피 부호화를 수행하는 경우 원본 데이터에 비해 최대 27.8%의 데이터를 사용하여 표현하는 것이 가능 

  - 엔트로피 예제 
   
  - 엔트로피 코딩 
    - 심볼이 나올 확률에 따라 심볼을 나타내는 코드의 길이를 다르게 함
    ## 일반적으로 기호의 불확실성에 비례하는 코드를 사용 
      - Uk = log2 1/Pk = -log2 Pk
      - 즉, 가장 자주 나오는 심볼에 대한 코드가 가장 짧음 

  - 텍스트 인코딩 예제 (1)
    - 다음과 같은 텍스트 문자정보를 ASCII 코드를 사용해 인코딩 
      - 모든 심볼은 동일한 코드워드 길이를 이용하므로, 데이터 압축이 되지 않음
      - 70비트 

  - 예제 (2)
    - 인코딩 시 문자열의 구성을 알 수 있는 경우
      - 문자열에 등장하는 심볼을 미리 파악
      - 해당 예제에는 오직 3가지 심볼(A,B,W)만 등장
        - 코드워드로 8비트를 사용할 필요가 없음
      - 3가지 종류의 심볼이 있으니 2비트를 사용해보면 어떨까??
        - 2비트 = 4가지의 정보를 표현 할 수 있음
  
  - 예제 (3)
    - 문자열의 분포를 알 수 있는 경우
      - 문자열에 등장하는 심볼과 그 분포를 파악
      - 많이 나오는 코드에는 짧은 코드워드를 줌 
     
   - 예제 (3) 디코딩
    - 앞에서 얻은 코드워드를 토대로 원본 데이터를 복원 

  - 고유 디코딩 (Unique decoding)
    - 고유 디코딩의 조건
      - 어떠한 코드워드도 다른 코드워드의 앞에 사용될 수 없다. (prefix)
      - 이진 트리 표현
        - 고유 디코딩 조건을 만족시키는 코드워드 집합은 이진 트리로 표현 가능함
      - 고유 디코딩을 만족 하지 않는 코드워드 예제 
        - s1 = 0, s2 = 10, s3 = 101 
        - 이유 : s3 contains s2 as its prefix 
        - it can not create binary decoding tree
        - 즉, 고유 디코딩 조건을 만족하지 않은경우, 코드워드는 다양한 심볼로 해석될 수 있다.
        - 아는 정보 상호 변경을 불가능하게 만듬

   - 예제 (4)

- 이번시간에는
  - 데이터 압축의 필요성
    - 많은 정보를 편하게 다루기 위함
    - 멀티미디어 데이터 저장에는 매우 많은 데이터가 필요
  - 압축의 정의
    - 인코더(encoder)를 통해 원본 데이터 심볼(symbol)에서부터 인코딩 된 코드워드의 평균 길이를 최소화 하는 과정
  - 정보 엔트로피
    - 정보 엔트로피는 데이터를 압축하는 데 필요한 최대 데이터양을 제시 
    - 불확실성이 커질수록 엔트로피는 커지고 포함하는 정보의 양은 증가 

- 다음시간에는
  - 대표적인 정보 인코딩 기법에 대해 배운다.





